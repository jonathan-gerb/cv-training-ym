{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Training YM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise format\n",
    "The sections of code that you will have to fill in yourself are marked by blocks starting with ##### and ending with #=======.\n",
    "\n",
    "#>>(notes)lines provide hints for the problem and #- lines are stand-ins for statements you need to fill in. You'll often find variables in the format ```variable = None```, these can be variables or objects where I've provided the name so they'll be consistent with code later in the exercise. \n",
    "\n",
    "These are just guidelines and if you have a different idea of how to approach the problem feel free to deviate. Try to think of what conceptually needs to happen to solve the problem and then implement it. Don't stay with one problem for too long if you get stuck. Instead, just look at the answers. Sometimes these problems come down to knowing some specific syntax and become a lot harder if you don't. It is more important that you get a feel for some of these concepts so have a starting point if you have to work with them for a project than that you solve all of on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Introduction to Convnets with pytorch \n",
    "In this section we will build a simple convolutional model and train it to classify images from the CIFAR-10 dataset. The CIFAR-10 dataset is one of the first large scale image datasets, but the images are very small (32x32). All convolutional models will do well on this if they have enough layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data using the torchvision library\n",
    "Pytorch has some build in libraries to download certain datasets, the CIFAR-10 dataset is one of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# convnet classification task on \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # you can add other transformations in this list if you want.\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#################\n",
    "# >> make cifar10 test and train dataset objects\n",
    "cifar10_train_dataset = None\n",
    "cifar10_test_dataset = None\n",
    "#================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make dataloaders from the datasets\n",
    "Wrap the dataset classes in a pytorch dataloader class. This will allow us to feed the data to the model in a easy and controllable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust batch size to fit gpu space AND for finetuning the training process, you'll have to do this for all different exercises.\n",
    "# batch size is not an insignificant factor in the training of convnets (or any neural networks for that matter)\n",
    "batch_size = 32\n",
    "\n",
    "#################\n",
    "# >> make train and test dataloader objects\n",
    "train_loader = None\n",
    "test_loader = None\n",
    "#================\n",
    "\n",
    "\n",
    "# the dataset class should contain the mapping of label to label_idx\n",
    "label_to_classname = {v: k for k, v in cifar10_test_dataset.class_to_idx.items()}\n",
    "\n",
    "# show single sample\n",
    "for batch, target in test_loader:\n",
    "    sample = batch[0].permute(1, 2, 0)\n",
    "    t_label = target[0].item()\n",
    "    print(f\"target label    : {label_to_classname[t_label]}\")\n",
    "    print(f\"shape of tensors: batch={batch.shape}, target={target.shape}\")\n",
    "    plt.imshow(sample.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build your convolutional model\n",
    "Use 2d Convolutional layers and ReLU layers to construct a simple neural network. You will also need a linear layer (also called fully connected layer) at the end and perhaps some dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerySimpleNet(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(VerySimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #################\n",
    "        # >> add a few layers, I suggest alternating conv2d and relu layers while increasing the amount of filters \n",
    "        \n",
    "        self.fc = None\n",
    "        #================\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        #################\n",
    "        # >> call layers in the forward pass\n",
    "        \n",
    "        #================\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing the model\n",
    "We'll now define basic training and testing routines. In pytorch you'll have to specify which optimizer, loss and scheduler (if you want to use one) you want to use and put the statements in the right spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've written the train and test methods for you, have a look at them and see if you understand what they do.\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch, log_interval=5, dry_run=False):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #################\n",
    "        # data and target to gpu\n",
    "        #-\n",
    "        \n",
    "        # reset gradients of previous iteration\n",
    "        #-\n",
    "        \n",
    "        # forward pass of model\n",
    "        #-\n",
    "        \n",
    "        # calculate loss\n",
    "        #-\n",
    "        \n",
    "        # calculate gradients\n",
    "        #-\n",
    "        \n",
    "        # apply optimizer based on gradients\n",
    "        #-\n",
    "        #================\n",
    "        # log the loss\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            #################\n",
    "            # data to gpu\n",
    "            #- \n",
    "            \n",
    "            # data through model\n",
    "            #- \n",
    "            # output = ...\n",
    "            # calculate loss (only for logging, we're not going to use it for backpropagation)\n",
    "            #- \n",
    "            \n",
    "            # add loss to total loss\n",
    "            test_loss += loss\n",
    "            \n",
    "            #================\n",
    "            # calculate metric\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "# some parameters\n",
    "device = \"cuda:0\"\n",
    "lr = 0.001\n",
    "gamma = 0.7\n",
    "epochs = 5\n",
    "model = VerySimpleNet(10).to(device) # transfer model to GPU\n",
    "\n",
    "#################\n",
    "# >> define optimizer, loss function (criterion) and scheduler\n",
    "optimizer = None\n",
    "criterion = None\n",
    "scheduler = None\n",
    "#================\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch, log_interval=100)\n",
    "    test(model, device, criterion, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the results\n",
    "Lets look at some classification examples from the test set, how does your model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, target in test_loader:\n",
    "    sample = batch[0].permute(1, 2, 0)\n",
    "    output = model(batch.to(device))\n",
    "    p_label = torch.argmax(output[0]).item()\n",
    "    t_label = target[0].item()\n",
    "    print(f\"target label: {label_to_classname[t_label]}, predicted label: {label_to_classname[p_label]}\")\n",
    "    plt.imshow(sample.numpy())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Convnets - Architecture, Backbones & Layers\n",
    "Backbones are large (often pretrained) neural networks that serve as a way to extract features that are useful for almost all types of problems.\n",
    "The general idea of a backbone is to have a model that is very good at identifying high level features in an image. \n",
    "The easiest way to understand this is to think of images as collections of shapes. A face is just a collection of circles (such as the eyes) and curvy lines.\n",
    "This means that if we already have a model that can detect all these components the step to the full combination of those components (such as a face) is a lot easier!\n",
    "In practice most pretrained backbones will already have concepts such as faces embedded into the layers. Additionally, lines and circles are actually pretty basic features and a large pretrained backbone will contain much more complex concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting CIFAR-10 \n",
    "Now we know about backbones, let's use one on the CIFAR-10 dataset. For this we'll need to download a pretrained model and adjust the number of classes. We'll pick the resnext50 backbone model, a variant of the resnet style architecture (see, https://pytorch.org/hub/pytorch_vision_resnext/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# >> import a pretrained backbone from the torchvision.models library\n",
    "\n",
    "pretrained_model = None\n",
    "#================\n",
    "\n",
    "# you can always print a model to see the full structure or even the partial structure if you select it.\n",
    "print(pretrained_model.layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing a backbone\n",
    "We now want to change the backbone so it fits our 10 class classification problem. There are a few ways to do this and here we will look at both of the methods.\n",
    "\n",
    "1. The first (and easiest) way is to change the model is to just swap out the last layer. In this case the original model was already a classification model and we are just changing the number of output classes in the last layer to fit our problem. In this case this is also the most 'correct' way of doing it.\n",
    "2. The second way to adjust the model is to wrap it in a new model class. This method is useful in more complicated scenarios where you want to change more than just the number of outputs. For example if we wanted to use the backbone as the basis for a segmentation model. Now before you ask, yes wrapping the backbone like this preserves the last layer that would be replaced in the other example, luckily this does not effect performance (only memory usage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: replacing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "#################\n",
    "# >> change the last layer of the model, (run the training cell further below to see if it worked):\n",
    "\n",
    "#================\n",
    "\n",
    "print(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: wrapping modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's wrap our model in a pytorch module instead, this is syntactically the same as adding a layer to a regular network.\n",
    "\n",
    "class WrappedResnext(nn.Module):\n",
    "    def __init__(self, n_classes, backbone):\n",
    "        super(WrappedResnext, self).__init__()\n",
    "        #################\n",
    "        self.backbone = None\n",
    "        self.fc2 = None\n",
    "        #================\n",
    "\n",
    "    def forward(self, x):\n",
    "        #################\n",
    "        # >> do forward pass\n",
    "        \n",
    "        #================\n",
    "        return x\n",
    "\n",
    "#################\n",
    "# >> import unchanged model again\n",
    "pretrained_model = None\n",
    "wrapped_model = None\n",
    "#================\n",
    "\n",
    "print(wrapped_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your adjusted models by running the cell below\n",
    "Now that we've adjusted our model for our problem we can try it out. Try both ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "gamma = 0.7\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "wrapped_model = wrapped_model.to(device)\n",
    "\n",
    "#################\n",
    "# >> define optimizer, loss function (criterion) and scheduler again. You've done this before, I'm just making you do it again.\n",
    "optimizer = None\n",
    "criterion = None\n",
    "scheduler = None\n",
    "#================\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch, log_interval=100)\n",
    "    test(model, device, criterion, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the results\n",
    "And? does the model perform better? can you even tell without training for a (very) long time?\n",
    "try looking at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see some samples\n",
    "for batch, target in test_loader:\n",
    "    sample = batch[0].permute(1, 2, 0)\n",
    "    output = model(batch.to(device))\n",
    "    p_label = torch.argmax(output[0]).item()\n",
    "    t_label = target[0].item()\n",
    "    print(f\"target label: {label_to_classname[t_label]}, predicted label: {label_to_classname[p_label]}\")\n",
    "    plt.imshow(sample.numpy())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start this section!\n",
    "We will be using another fairly large dataset in this section so turn on the download by running the imports and the *Testing the model* section "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility: clearing GPU memory\n",
    "when working with pytorch or any deep learning framework you will likely get errors concerning the GPU memory. Take a look at the code below, running this method (or something similar) can help clearing the GPU memory if this becomes a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc # gc = garbage collection\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "            if obj.size() == torch.Size([4, 21, 568, 568]):\n",
    "                del obj\n",
    "            if obj.size() == torch.Size([4, 3, 568, 568]):\n",
    "                del obj\n",
    "            if obj.size() == torch.Size([4, 1, 568, 568]):\n",
    "                del obj\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Understanding Convnets - Model architecture\n",
    "In this section we'll look at Unet, a classic (compared to more recent models) deep learning segmentation model that introduced the concept of encoder-decoder architecture. It is also fairly easy to understand. It consists of two parts, the encoder and the decoder. The Encoder has the same job as the backbone has in the previous section: Its purpose is to extract features (you can thing of the as classifications or objects) from the image, that will then be used in the second half of the model (the decoder) to make the segmentation, (place those objects in the right place with the right boundries). In this sense, the Decoder does the same thing as the single layer of linear nodes in the backbone example. It uses the features supplied by the encoder to make a classification, just for all pixels in the image instead of the entire image. \n",
    "\n",
    "So thats the general idea, but what are some of the other interesting parts of the model? \n",
    "1. As we can see in the image below, the shape of the layers gets deeper in filters but smaller in width/height.\n",
    "This is done to allow the model to learn larger more complex concepts. As the size of the convolutional filters stays the same throughout the model (generally kernel size is always 3x3), a larger object like a car would never fit in those 3 pixels. By downsizing the output after each layer, a full object CAN be represented in that 3x3 grid of pictures. \n",
    "This is because filters specialize, a certain filter in the 4th layer of the model might only respond to a specific pattern that was found in the previous layer. That pattern is again already an abstraction upon the previous input etc. etc. until you reach the first layer where only lines and squiggles are detected.\n",
    "\n",
    "2. What are those grey lines jumping from the enconder part to the decoder part? As you might have suspected this is just the output of that layer being saved and copied to be concatenated to the decoder at the mirrored layer later. This is done because a lot of spatial detail is lost in the process of downsampling. By adding the higher resolution data from the downsampling process this effect is somewhat mitigated as the network is able to use this data to make more precise object boundaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://miro.medium.com/max/2824/1*f7YOaE4TWubwaFF7Z1fzNw.png \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the segmentation model\n",
    "I've defined some functions to get you started, try to complete the model based on the architecture shown above. you can use a linear layer at the end but you don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports so you don't have to scroll up when you get an OOM error\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From classification model to segmentation model\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )   \n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()  \n",
    "        #################\n",
    "        # >> finish the encoder and then make the decoder\n",
    "        # encoder\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = None\n",
    "        self.dconv_down4 = None\n",
    "\n",
    "        # up\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # down\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        \n",
    "        # decoder\n",
    "        # >> add layers for the decoder yourself \n",
    "        \n",
    "        \n",
    "        # >> final layer\n",
    "        self.final_layer = None\n",
    "        #================\n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        #################\n",
    "        # >> finish the encoder forward pass\n",
    "        \n",
    "        \n",
    "        # >> add the decoder forward pass\n",
    "        \n",
    "        #================\n",
    "        \n",
    "        out = self.final_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model \n",
    "Lets try out our new model on another pytorch/torchvision dataset. We'll use the SBD dataset, another dataset that can be downloaded using the torchvision dataset library.\n",
    "This dataset supplies full segmentations instead of just classes. Its a little trickier to use so I've completed this section for you, feel free to look around though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# custom compose object to transform the pil images to tensors in the right format\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image = t(image)\n",
    "            if type(t) != transforms.Normalize:\n",
    "                target = t(target)\n",
    "            \n",
    "        return image, target\n",
    "\n",
    "transform = []\n",
    "transform.append(transforms.Resize((568, 568), interpolation=Image.NEAREST))\n",
    "transform.append(transforms.ToTensor())\n",
    "transform.append(transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      std=[0.229, 0.224, 0.225]))\n",
    "transform = Compose(transform)\n",
    "\n",
    "# sadly this dataset didnt have the classes attribute so I had to add them manually.\n",
    "label_to_classname = {\n",
    "    0: \"background\",\n",
    "    1: \"aeroplane\",\n",
    "    2: \"bicycle\",\n",
    "    3: \"bird\",\n",
    "    4: \"boat\",\n",
    "    5: \"bottle\",\n",
    "    6: \"bus\",\n",
    "    7: \"car\",\n",
    "    8: \"cat\",\n",
    "    9: \"chair\",\n",
    "    10: \"cow\",\n",
    "    11: \"diningtable\",\n",
    "    12: \"dog\",\n",
    "    13: \"horse\",\n",
    "    14: \"motorbike\",\n",
    "    15: \"person\",\n",
    "    16: \"pottedplant\",\n",
    "    17: \"sheep\",\n",
    "    18: \"sofa\",\n",
    "    19: \"train\",\n",
    "    20: \"tvmonitor\"\n",
    "}\n",
    "\n",
    "# this dataset is 1.4 Gigabyte so be patient. \n",
    "sbd_dataset = torchvision.datasets.SBDataset(root=\"./sbd\", download=False, mode=\"segmentation\", image_set='train', transforms=transform)\n",
    "\n",
    "# pick the right batch_size, generally you would want to use at least 16 for any convnet training, \n",
    "# but often this will be impossible due to the size on smaller gpus, we're really only trying stuff out so you can use a smaller size if needed here\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(sbd_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the data\n",
    "Run this a few times to look at some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, target_batch in train_loader:\n",
    "    image = image_batch[0]\n",
    "    target = target_batch[0]\n",
    "    \n",
    "    image_cl = image.permute(1, 2, 0)\n",
    "    target_cl = target.permute(1, 2, 0)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20,10))\n",
    "    \n",
    "    # We are plotting normalized images here, you could turn denormalize them with the values in the transform operation, but for the purpose of seeing\n",
    "    #the content of the image this is just fine\n",
    "    axs[0].imshow(image_cl.numpy())\n",
    "    \n",
    "    # naive approach to muticlass plotting, classes are assigned different colors due to the values being different, but is not consistent between images\n",
    "    axs[1].imshow(target_cl.numpy())\n",
    "    print(f\"classes: {[label_to_classname[c] for c in np.unique(target_cl.numpy()*255)]}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "Training the segmentation model is basically the same as for any neural network. Just plug in the data and let the optimizer do the work. Fitting this model is a pretty heavy task and it might take too long, I've added an overfit function that will just overfit the model on 1 image, things like this are a good way of testing whether the model is capable of processing this type of data. For the model to perfectly overfit an image the whole training routine has to work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch, log_interval=5, dry_run=False, overfitrun=False):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    n_iters = 0\n",
    "    overfitdata = None\n",
    "    overfittarget = None\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if overfitrun:\n",
    "            if batch_idx == 0:\n",
    "                overfitdata = data \n",
    "                overfittarget = target\n",
    "            else:\n",
    "                data = overfitdata\n",
    "                target = overfittarget\n",
    "            \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        target = target.reshape(batch_size, 568, 568) * 255\n",
    "        target = target.long()\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "        n_iters += 1\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t average Loss (last 500): {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), np.mean(losses[-500:])))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "# parameters\n",
    "device = \"cuda:0\"\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "gamma = 0.7\n",
    "\n",
    "#################\n",
    "# >> this time write the training code yourself (its still more or less the same as before)) \n",
    "# >> make the model\n",
    "\n",
    "# >> define optimizer, loss function and scheduler\n",
    "\n",
    "# >> do the train and test step for n epochs\n",
    "\n",
    "    \n",
    "#================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can save the model if you want or load one I pretrained\n",
    "If your model is taking a while to train, you can copy the example model code and import these pretrained weights to see some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "# torch.save(segmentation_model.state_dict(), \"segmentation_model.pt\")\n",
    "\n",
    "# load model\n",
    "# segmentation_model = UNet(sbd_dataset.num_classes + 1)\n",
    "# segmentation_model.load_state_dict(torch.load(\"segmentation_model.pt\"))\n",
    "# segmentation_model.eval()\n",
    "# segmentation_model = segmentation_model.to(device)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the results\n",
    "You can see the results of your training here. keep in mind however that it takes a long time to train a large model like this. You won't get good looking results unless you leave it running for a while. Try the pretrained backbone (look in the answers to find my implementation that fits it) for a working(-ish) example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, target_batch in train_loader:\n",
    "    #################\n",
    "    # >> extract the first image in the batch and its target\n",
    "    \n",
    "    # >> switch channel orders for plotting\n",
    "    \n",
    "    # >> pass the batch through the model and get the predictions\n",
    "    \n",
    "    # >> get the predictions for the first image and transform them for plotting \n",
    "    \n",
    "    #================\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20,10))\n",
    "    axs[0].imshow(image_cl.numpy())\n",
    "    \n",
    "    # naive approach to muticlass plotting, classes are assigned different colors due to the values being different, but is not consistent between images\n",
    "    axs[1].imshow(target_cl.numpy())\n",
    "    axs[2].imshow(pred.numpy())\n",
    "    print(f\"classes: {[label_to_classname[c] for c in np.unique(target_cl.numpy()*255)]}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Understanding Convnets - Filter Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting area of research within convolutional neural networks is the interpretability. As you will likely know, neural networks are mostly black box solutions. It is hard to get an idea of why the network performs the way it does. There are several ways in which more insight can be gained from the model, but most of them focus on various ways of tracking the activation of the model on a certain image. Doing this for a large amount of images gives insight into what parts of the model respond to specific stimuly. This process is somewhat similar to how human or animal brains can be studied as well, if you are shown multiple similar pictures, most likely the same area of the brain will be used. \n",
    "\n",
    "Using neural networks however, we can do more than just track the activation throught the network. Neural networks, although they are large black boxes, are deterministic. This means that we always get the same output for the same image, but more interesting, this means we can make small adjustments to the input image and by doing so map the internal logic of the network! \n",
    "\n",
    "In this example we will apply a process called deep-dreaming (https://en.wikipedia.org/wiki/DeepDream) to see inside a neural network. More specifically we will change the optimization goal to visualize the convolutional filters. We will do so by inputting random noise and adjusting that noise to get a higher activation of a specific filter/layer. Adjusting values to get a higher activation? does that sound familiar? Well it should because this method uses the same backpropagation algorithm as regular training just with a different target!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our model\n",
    "For this task we will use a pretrained model. This is because a pretrained model will already have well trained filters that look for very specific patterns. If we were to use a untrained model we might not get good visualizations at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use the same backbone model as before\n",
    "\n",
    "#################\n",
    "# >> import pretrained model\n",
    "model = None\n",
    "\n",
    "#================\n",
    "\n",
    "# set in eval mode to avoid adjusting the weights\n",
    "model.eval()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the layers & modules in the model and pick one as optimization target\n",
    "# list(resnext50.modules())\n",
    "model.layer2[1].conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving intermediate results\n",
    "To do the optimization we need the output of a specific layer/module in the model. We can do this in pytorch by making use of a Hook. A hook will be attached to a specific location in the model and will save automatically save what we want when the models forward function is called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook():\n",
    "    def __init__(self, module, backward=False):\n",
    "        if backward==False:\n",
    "            self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        else:\n",
    "            self.hook = module.register_backward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        \n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting gradients for optimization\n",
    "Instead of optimizing using the loss gained by comparing the output to the target label, our loss will just be the activation of the layer that we set as target. Additionally we will be trying to get the loss as high as possible instead of low like in a regular training setup, this is called gradient ascent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to make gradients calculations from the output channels of the target layer  \n",
    "def get_gradients(model_input, model, layer, filter_idx):     \n",
    "    model_input = model_input.unsqueeze(0).cuda() # some reshaping\n",
    "    \n",
    "    #################\n",
    "    # >> fill in these one line statements at the #-\n",
    "    \n",
    "    # we want to get gradients from the forward pass so make sure the inpute data generates gradients\n",
    "    #-\n",
    "    \n",
    "    # discard any previous gradients\n",
    "    #-\n",
    "    \n",
    "    # apply the hook we made earlier to the target layer\n",
    "    #-\n",
    "    \n",
    "    # do the forward pass, we won't actually use the final output\n",
    "    #-\n",
    "    \n",
    "    # get the loss by retrieving the output saved by the hook.\n",
    "    # we will take the norm of the output because we want extreme values in both directions, positive AND negative.\n",
    "    # don't forget that the data is in batched format, even though we're only supplying one example \n",
    "    #-\n",
    "    \n",
    "    # use the loss to calculate the gradients\n",
    "    #-\n",
    "    \n",
    "    # return the gradients we just calculated (and reshape data)\n",
    "    return None\n",
    "    #================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Ascent\n",
    "Now that we have a way to get specific outputs from inside the model and a way to calculate gradients for our optimization target, we can write the full deep dream function.\n",
    "In this function we will prepare the image, perform the gradient ascent and return the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denormalization image transform, used to give the image the right colors again.\n",
    "denorm = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]),                 \n",
    "                             transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ]),                                                     \n",
    "                            ])\n",
    "\n",
    "def dream(image, model, layer, iterations, lr, filter_idx):\n",
    "    \"\"\"Deep dream an image using given model and parameters.\"\"\"\n",
    "    #################\n",
    "    # >> fill in these one line statements at the #-\n",
    "    \n",
    "    # convert image to tensor\n",
    "    #-\n",
    "    \n",
    "    # remove additional channels if it's present (pngs will have a 4th transparancy channel)\n",
    "    #-\n",
    "    \n",
    "    # normalize the image\n",
    "    image_tensor = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image_tensor).cuda()\n",
    "    \n",
    "    # deep dream the image\n",
    "    for i in range(iterations):\n",
    "        # get gradients \n",
    "        gradients = get_gradients(image_tensor, model, layer, filter_idx)\n",
    "        \n",
    "        # add gradients to image to morph the image in a direction that causes a higher activation, we'll add a learning rate parameter to control the effect\n",
    "        #-\n",
    "        \n",
    "    #================\n",
    "    # get the final image from gpu\n",
    "    img_out = image_tensor.detach().cpu()\n",
    "    \n",
    "    # denormalize\n",
    "    img_out = denorm(img_out)\n",
    "    \n",
    "    # do some reshaping, conversion\n",
    "    img_out_np = img_out.numpy().transpose(1,2,0)\n",
    "    img_out_np = np.clip(img_out_np, 0, 1)\n",
    "    img_out_pil = Image.fromarray(np.uint8(img_out_np * 255))\n",
    "    return img_out_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Dreaming \n",
    "Now that we have all our functions, let's try them out on a bunch of images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "# Get an input image, I've used weblinks here but you can upload your own as well, you could even try using random noise!\n",
    "url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Kladsk%C3%A1_forrest.jpg/1024px-Kladsk%C3%A1_forrest.jpg'\n",
    "# url = 'https://www.marineterrein.nl/wp-content/uploads/2019/02/IMG_8815-830x466.jpg'\n",
    "# url = 'https://youngmavericks.com/img/contact_location.png'\n",
    "\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# might want to resize if they're very large, it would take a while to do the whole deep dream\n",
    "orig_size = np.array(img.size)\n",
    "# new_size = np.array(img.size)*0.5\n",
    "\n",
    "#################\n",
    "\n",
    "# make sure the model is on the gpu\n",
    "#-\n",
    "\n",
    "# pick a target layer\n",
    "#-\n",
    "\n",
    "# set some parameters\n",
    "filter_idx = None\n",
    "learning_rate = None\n",
    "iterations = None\n",
    "\n",
    "# call the deep dream function on an image and get the deep dreamed image\n",
    "# -\n",
    "\n",
    "#================\n",
    "\n",
    "# resize to original if needed \n",
    "img = img.resize(orig_size)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize = (20 , 20))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convnets for spatially related non-image data\n",
    "### Downloading the data.\n",
    "The data for this section is quite large so before going any further, start the download :). Download the data here. https://drive.google.com/file/d/1r_SUJpfz3qX0j6ZwmFwCBTEHw7EE-q4l/view?usp=sharing & https://drive.google.com/file/d/1GO6Stq_eRsJGaQL8KVWoex1A2vcX4D87/view?usp=sharing\n",
    "\n",
    "dataset site:\n",
    "http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/#downloads\n",
    "\n",
    "### Overview \n",
    "Convnets are typically associated with image classification tasks, however any spatially related data can be classified with convnets. In this example we will see how it possible to classify audio data with convnets. More specifically, we will try to classify short audio samples as contains birdsong or not. \n",
    "To do this, we'll first have to convert audio data into something that can be processed using a neural net. Spectograms (https://en.wikipedia.org/wiki/Spectrogram) are images that show the frequencies present in sound data over a period of time. These images will then be fed to the convnet together with the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the spectograms\n",
    "To make the spectograms we will make use of the scipy signal processing library, this is pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "#################\n",
    "\n",
    "# Read the wav file (stereo)\n",
    "# - \n",
    "\n",
    "# convert to mono if needed and reshape, the bird data should already be mono\n",
    "#-\n",
    "\n",
    "# crop to 10 seconds (44kHz audio), it should be 10s already anyway\n",
    "#-\n",
    "\n",
    "#make the spectogram using the signal library\n",
    "#- frequencies, times, spectrogram = ... \n",
    "\n",
    "#================\n",
    "# draw the image\n",
    "plt.pcolormesh(times, frequencies, np.log(spectrogram), shading='auto')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters\n",
    "#################\n",
    "basedir = \"path/to/spectogram/save/folder/\" # i suggest making a spectogram/ folder to save the images in \n",
    "filenames = glob(\"/path/to/wav/folder/*.wav\")\n",
    "#================\n",
    "\n",
    "os.makedirs(basedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all samples to spectograms\n",
    "We will now use this sample process to convert all audio samples to spectograms, to speed it up, lets use the python multiprocessing pool function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def make_spectogram(filename):\n",
    "    #################\n",
    "\n",
    "    # Read the wav file (stereo)\n",
    "    # - \n",
    "\n",
    "    # convert to mono if needed and reshape, the bird data should already be mono\n",
    "    #-\n",
    "\n",
    "    # crop to 10 seconds (44kHz audio), it should be 10s already anyway\n",
    "    #-\n",
    "\n",
    "    #make the spectogram using the signal library\n",
    "    #- frequencies, times, spectrogram = ... \n",
    "\n",
    "    #================\n",
    "\n",
    "    # To make a figure without the frame :\n",
    "    fig = plt.figure(frameon=False)\n",
    "    w = 10\n",
    "    h = 5\n",
    "    fig.set_size_inches(w,h)\n",
    "\n",
    "    # To make the content fill the whole figure\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "\n",
    "    # draw the image\n",
    "    plt.pcolormesh(times, frequencies, np.log(spectrogram), shading='auto')\n",
    "    dpi = 50\n",
    "    savename = os.path.join(basedir, os.path.basename(filename).split(\".\")[0] + \".jpg\")\n",
    "    fig.savefig(savename, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "\n",
    "# turn of plot generation\n",
    "plt.ioff()\n",
    "\n",
    "# this will take a few minutes.\n",
    "p = Pool(4) # adjust number of cores to desired amount or use os.cpu_count()\n",
    "with p:\n",
    "    p.map(make_spectogram, filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the pytorch dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the labels\n",
    "Labels are given in a csv file, each row contains the name of the sample and the classification (bird or notbird), we can read them using pandas and split them up into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "csv_file = '/home/parting/Downloads/warblrb10k_public/warblrb10k_public_metadata.csv'\n",
    "device = \"cuda:0\"\n",
    "train_test_split = 0.8\n",
    "\n",
    "#################\n",
    "# read csv with pandas\n",
    "df = None\n",
    "\n",
    "# split up the dataframe into train and test\n",
    "train_df = None\n",
    "test_df = None\n",
    "\n",
    "#================\n",
    "print(f\"length of trainset: {len(train_df)}, length of test set: {len(test_df)}\")\n",
    "\n",
    "# example\n",
    "print(\"example:\")\n",
    "print(f\"    spectogram with key: ({'759808e5-f824-401e-9058'}) has class: ({int(df.loc['759808e5-f824-401e-9058'][1])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch dataset class\n",
    "To load and preprocess all the spectograms we will need a new pytorch dataset class, we will then wrap this dataset class in a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from skimage import io, transform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "#################\n",
    "class BirdDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # setup all the object attributes\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the amount of samples\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        #- read the image\n",
    "        \n",
    "        # -get the label\n",
    "\n",
    "        #- normalize the image\n",
    "\n",
    "        # return the image, target and image name (for debuggin later)\n",
    "        return image, label, img_name\n",
    "\n",
    "#================\n",
    "# some standard transforms\n",
    "transform = transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#################\n",
    "# make the train and test datasets\n",
    "birddataset_train = None\n",
    "birddataset_test = None\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# wrap them in dataloaders\n",
    "train_loader = None\n",
    "test_loader = None\n",
    "\n",
    "#================\n",
    "# try out the dataset\n",
    "sample, label, fname = birddataset_train.__getitem__(0)\n",
    "print(f\"sample shape: {sample.shape}, label: {label}, filepath: {fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing the model\n",
    "We'll use basically the same code that we used to train on the cifar-10 and segmentation datasets, so I'm not going to give you a lot of help here. If you're curious, the original paper achieved about 88% accuracy on this dataset, how close can you get ;)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch, log_interval=5, dry_run=False):\n",
    "    model.train()\n",
    "    # do training\n",
    "\n",
    "\n",
    "def test(model, device, criterion, test_loader):\n",
    "    model.eval()\n",
    "    # do testing\n",
    "    \n",
    "\n",
    "lr = 0.001\n",
    "gamma = 0.7\n",
    "epochs = 5\n",
    "\n",
    "# prepare a pretrained model to train and use.\n",
    "\n",
    "\n",
    "# define optimizer, loss function, scheduler\n",
    "\n",
    "# train, test after each epoch\n",
    "\n",
    "#================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "# torch.save(resnext50.state_dict(), \"bird_class_model.pt\")\n",
    "\n",
    "# load model\n",
    "# resnext50 = models.resnext50_32x4d()\n",
    "# resnext50.fc = nn.Linear(2048, birddataset_train.n_classes)\n",
    "# resnext50.load_state_dict(torch.load(\"bird_class_model.pt\"))\n",
    "# resnext50.eval()\n",
    "# resnext50 = resnext50.to(device)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the results\n",
    "Now that everything is working, lets look at some examples, you probably can't play the sounds in this notebook, so open them outside of the notebook (download them if needed) if you want to get a feel for what the model can and can't do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(image_batch, target_batch, fnames):\n",
    "    image = image_batch[0]\n",
    "    image2 = image_batch[1]\n",
    "    target = target_batch[0]\n",
    "    \n",
    "    image = image.permute(1, 2, 0)\n",
    "    image2 = image2.permute(1, 2, 0)\n",
    "    image_batch = image_batch.to(device)\n",
    "    output = resnext50(image_batch).cpu()\n",
    "    image_batch = image_batch.cpu()\n",
    "    \n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    \n",
    "    # we're plotting normalized images instead of the originals, but it gets the point across I think\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20,10))\n",
    "    axs[0].imshow(image.numpy())\n",
    "    axs[0].set_title(f\"class: {target_batch[0]}, predicted: {pred[0].item()},\\n filepath: {fnames[0]}\")\n",
    "    axs[1].imshow(image2.numpy())\n",
    "    axs[1].set_title(f\"class: {target_batch[1]}, predicted: {pred[1].item()},\\n filepath: {fnames[1]}\")\n",
    "    print(fnames[0], fnames[1])\n",
    "\n",
    "for image_batch, target_batch, fnames in test_loader:\n",
    "    plot(image_batch, target_batch, fnames) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outro\n",
    "Well... you made it to the end of the exercises! Hope you enjoyed working on them and learned something ;). \n",
    "Some suggestions for follow up excersis:\n",
    "- deep dream the various models that haven't been deepdreamed, you could even try deepdreaming the bird detector.\n",
    "- try different backbones and see how large models actually have to be for the various problems, does it make a difference which one you use or are they pretty similar?\n",
    "- try deep dreaming whole layers and try layers early or late into the model, is it also possible to optimize for two targets at very different locations in the model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch16]",
   "language": "python",
   "name": "conda-env-pytorch16-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
