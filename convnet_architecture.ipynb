{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Training YM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Introduction to Convnets with pytorch \n",
    "In this section will will build a simple convolutional model and train it to classify images from the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data using the torchvision library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# convnet classification task on \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "cifar10_train_dataset = torchvision.datasetsw.CIFAR10(\"./cifar10\", download=True, transform=transform, train=True)\n",
    "cifar10_test_dataset = torchvision.datasets.CIFAR10(\"./cifar10\", download=True, transform=transform, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make dataloaders from the datasets\n",
    "Wrap the dataset classes in a pytorch dataloader class. This will allow us to feed the data to the model in a easy and controllable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target label    : ship\n",
      "shape of tensors: batch=torch.Size([256, 3, 32, 32]), target=torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adjust batch size to fit gpu space AND for finetuning the training process.\n",
    "# batch size is not an insignificant factor in the training of convnets (or any neural networks for that matter)\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=5)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(cifar10_test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=5)\n",
    "\n",
    "# the dataset class should contain the mapping of label to label_idx\n",
    "label_to_classname = {v: k for k, v in cifar10_test_dataset.class_to_idx.items()}\n",
    "\n",
    "# show single sample\n",
    "for batch, target in test_loader:\n",
    "    sample = batch[0].permute(1, 2, 0)\n",
    "    t_label = target[0].item()\n",
    "    print(f\"target label    : {label_to_classname[t_label]}\")\n",
    "    print(f\"shape of tensors: batch={batch.shape}, target={target.shape}\")\n",
    "    plt.imshow(sample.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "    \n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.out = nn.Linear(2048, 10)  # 512 * 4 * 4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x5 = torch.flatten(x5, 1)\n",
    "        logits = self.out(x5)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class VerySimpleNet(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(VerySimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, 1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.conv5 = nn.Conv2d(256, 512, 3, 1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc = nn.Linear(247808, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.302588\n",
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 2.214403\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.084307\n",
      "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 2.036350\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 1.973930\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.863580\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.820063\n",
      "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 1.818110\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.772925\n",
      "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.714329\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.800814\n",
      "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 1.701640\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.648644\n",
      "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 1.604365\n",
      "Train Epoch: 1 [35840/50000 (71%)]\tLoss: 1.516292\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.410293\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 1.504500\n",
      "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 1.432383\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 1.488869\n",
      "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 1.503304\n",
      "\n",
      "Test set: Average loss: 0.0058, Accuracy: 23402/50000 (46.80%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.478263\n",
      "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.420007\n",
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.325551\n",
      "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 1.427113\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.305202\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.244111\n",
      "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 1.316939\n",
      "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 1.346223\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.261771\n",
      "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 1.347105\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.392281\n",
      "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 1.237744\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.298378\n",
      "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 1.164221\n",
      "Train Epoch: 2 [35840/50000 (71%)]\tLoss: 1.305414\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.183832\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.228225\n",
      "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.275344\n",
      "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 1.155196\n",
      "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 1.300498\n",
      "\n",
      "Test set: Average loss: 0.0046, Accuracy: 28853/50000 (57.71%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.231093\n",
      "Train Epoch: 3 [2560/50000 (5%)]\tLoss: 1.073703\n",
      "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 1.117391\n",
      "Train Epoch: 3 [7680/50000 (15%)]\tLoss: 1.143006\n",
      "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 1.033351\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.083441\n",
      "Train Epoch: 3 [15360/50000 (31%)]\tLoss: 1.082392\n",
      "Train Epoch: 3 [17920/50000 (36%)]\tLoss: 1.066693\n",
      "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 1.203673\n",
      "Train Epoch: 3 [23040/50000 (46%)]\tLoss: 1.082834\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.056777\n",
      "Train Epoch: 3 [28160/50000 (56%)]\tLoss: 1.114864\n",
      "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 1.029834\n",
      "Train Epoch: 3 [33280/50000 (66%)]\tLoss: 1.096531\n",
      "Train Epoch: 3 [35840/50000 (71%)]\tLoss: 1.140547\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.168434\n",
      "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 1.054000\n",
      "Train Epoch: 3 [43520/50000 (87%)]\tLoss: 1.025466\n",
      "Train Epoch: 3 [46080/50000 (92%)]\tLoss: 1.100484\n",
      "Train Epoch: 3 [48640/50000 (97%)]\tLoss: 1.077897\n",
      "\n",
      "Test set: Average loss: 0.0043, Accuracy: 31031/50000 (62.06%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.085633\n",
      "Train Epoch: 4 [2560/50000 (5%)]\tLoss: 1.082767\n",
      "Train Epoch: 4 [5120/50000 (10%)]\tLoss: 1.224841\n",
      "Train Epoch: 4 [7680/50000 (15%)]\tLoss: 0.945247\n",
      "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 0.975177\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.059330\n",
      "Train Epoch: 4 [15360/50000 (31%)]\tLoss: 1.009249\n",
      "Train Epoch: 4 [17920/50000 (36%)]\tLoss: 1.031401\n",
      "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 1.205155\n",
      "Train Epoch: 4 [23040/50000 (46%)]\tLoss: 1.052789\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.965159\n",
      "Train Epoch: 4 [28160/50000 (56%)]\tLoss: 1.028571\n",
      "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 1.096408\n",
      "Train Epoch: 4 [33280/50000 (66%)]\tLoss: 1.120017\n",
      "Train Epoch: 4 [35840/50000 (71%)]\tLoss: 0.979752\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.088368\n",
      "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 0.941580\n",
      "Train Epoch: 4 [43520/50000 (87%)]\tLoss: 0.929177\n",
      "Train Epoch: 4 [46080/50000 (92%)]\tLoss: 1.124293\n",
      "Train Epoch: 4 [48640/50000 (97%)]\tLoss: 1.021266\n",
      "\n",
      "Test set: Average loss: 0.0038, Accuracy: 33072/50000 (66.14%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.014091\n",
      "Train Epoch: 5 [2560/50000 (5%)]\tLoss: 0.908699\n",
      "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 1.066269\n",
      "Train Epoch: 5 [7680/50000 (15%)]\tLoss: 0.985636\n",
      "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 0.863691\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.050950\n",
      "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 1.020002\n",
      "Train Epoch: 5 [17920/50000 (36%)]\tLoss: 0.840088\n",
      "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 0.926914\n",
      "Train Epoch: 5 [23040/50000 (46%)]\tLoss: 1.038822\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.960628\n",
      "Train Epoch: 5 [28160/50000 (56%)]\tLoss: 0.904575\n",
      "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 0.955875\n",
      "Train Epoch: 5 [33280/50000 (66%)]\tLoss: 0.957367\n",
      "Train Epoch: 5 [35840/50000 (71%)]\tLoss: 0.983014\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.944785\n",
      "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 0.854184\n",
      "Train Epoch: 5 [43520/50000 (87%)]\tLoss: 0.759231\n",
      "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 0.904270\n",
      "Train Epoch: 5 [48640/50000 (97%)]\tLoss: 0.961354\n",
      "\n",
      "Test set: Average loss: 0.0034, Accuracy: 34800/50000 (69.60%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.825998\n",
      "Train Epoch: 6 [2560/50000 (5%)]\tLoss: 0.845702\n",
      "Train Epoch: 6 [5120/50000 (10%)]\tLoss: 0.820438\n",
      "Train Epoch: 6 [7680/50000 (15%)]\tLoss: 0.878273\n",
      "Train Epoch: 6 [10240/50000 (20%)]\tLoss: 0.801650\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.875688\n",
      "Train Epoch: 6 [15360/50000 (31%)]\tLoss: 0.771739\n",
      "Train Epoch: 6 [17920/50000 (36%)]\tLoss: 0.836712\n",
      "Train Epoch: 6 [20480/50000 (41%)]\tLoss: 0.884976\n",
      "Train Epoch: 6 [23040/50000 (46%)]\tLoss: 0.954997\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.936962\n",
      "Train Epoch: 6 [28160/50000 (56%)]\tLoss: 0.829004\n",
      "Train Epoch: 6 [30720/50000 (61%)]\tLoss: 0.919796\n",
      "Train Epoch: 6 [33280/50000 (66%)]\tLoss: 0.912791\n",
      "Train Epoch: 6 [35840/50000 (71%)]\tLoss: 0.919942\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.875785\n",
      "Train Epoch: 6 [40960/50000 (82%)]\tLoss: 0.932750\n",
      "Train Epoch: 6 [43520/50000 (87%)]\tLoss: 0.712027\n",
      "Train Epoch: 6 [46080/50000 (92%)]\tLoss: 0.827251\n",
      "Train Epoch: 6 [48640/50000 (97%)]\tLoss: 0.963006\n",
      "\n",
      "Test set: Average loss: 0.0032, Accuracy: 35867/50000 (71.73%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.852572\n",
      "Train Epoch: 7 [2560/50000 (5%)]\tLoss: 0.906663\n",
      "Train Epoch: 7 [5120/50000 (10%)]\tLoss: 0.930162\n",
      "Train Epoch: 7 [7680/50000 (15%)]\tLoss: 0.836115\n",
      "Train Epoch: 7 [10240/50000 (20%)]\tLoss: 0.879087\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.890903\n",
      "Train Epoch: 7 [15360/50000 (31%)]\tLoss: 0.787234\n",
      "Train Epoch: 7 [17920/50000 (36%)]\tLoss: 0.792773\n",
      "Train Epoch: 7 [20480/50000 (41%)]\tLoss: 0.828532\n",
      "Train Epoch: 7 [23040/50000 (46%)]\tLoss: 0.766783\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.846824\n",
      "Train Epoch: 7 [28160/50000 (56%)]\tLoss: 0.899986\n",
      "Train Epoch: 7 [30720/50000 (61%)]\tLoss: 0.773044\n",
      "Train Epoch: 7 [33280/50000 (66%)]\tLoss: 0.774244\n",
      "Train Epoch: 7 [35840/50000 (71%)]\tLoss: 1.028762\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.872071\n",
      "Train Epoch: 7 [40960/50000 (82%)]\tLoss: 0.809053\n",
      "Train Epoch: 7 [43520/50000 (87%)]\tLoss: 0.905023\n",
      "Train Epoch: 7 [46080/50000 (92%)]\tLoss: 0.752596\n",
      "Train Epoch: 7 [48640/50000 (97%)]\tLoss: 0.937121\n",
      "\n",
      "Test set: Average loss: 0.0032, Accuracy: 35845/50000 (71.69%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.832259\n",
      "Train Epoch: 8 [2560/50000 (5%)]\tLoss: 0.767710\n",
      "Train Epoch: 8 [5120/50000 (10%)]\tLoss: 0.779108\n",
      "Train Epoch: 8 [7680/50000 (15%)]\tLoss: 0.776992\n",
      "Train Epoch: 8 [10240/50000 (20%)]\tLoss: 0.734021\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.841411\n",
      "Train Epoch: 8 [15360/50000 (31%)]\tLoss: 0.866129\n",
      "Train Epoch: 8 [17920/50000 (36%)]\tLoss: 0.786805\n",
      "Train Epoch: 8 [20480/50000 (41%)]\tLoss: 0.863895\n",
      "Train Epoch: 8 [23040/50000 (46%)]\tLoss: 0.907430\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.723768\n",
      "Train Epoch: 8 [28160/50000 (56%)]\tLoss: 0.829980\n",
      "Train Epoch: 8 [30720/50000 (61%)]\tLoss: 0.759201\n",
      "Train Epoch: 8 [33280/50000 (66%)]\tLoss: 0.846323\n",
      "Train Epoch: 8 [35840/50000 (71%)]\tLoss: 0.789789\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.739820\n",
      "Train Epoch: 8 [40960/50000 (82%)]\tLoss: 0.835646\n",
      "Train Epoch: 8 [43520/50000 (87%)]\tLoss: 0.898840\n",
      "Train Epoch: 8 [46080/50000 (92%)]\tLoss: 0.818780\n",
      "Train Epoch: 8 [48640/50000 (97%)]\tLoss: 0.754066\n",
      "\n",
      "Test set: Average loss: 0.0031, Accuracy: 36490/50000 (72.98%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.922486\n",
      "Train Epoch: 9 [2560/50000 (5%)]\tLoss: 0.926040\n",
      "Train Epoch: 9 [5120/50000 (10%)]\tLoss: 0.679536\n",
      "Train Epoch: 9 [7680/50000 (15%)]\tLoss: 0.631694\n",
      "Train Epoch: 9 [10240/50000 (20%)]\tLoss: 0.776897\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.817239\n",
      "Train Epoch: 9 [15360/50000 (31%)]\tLoss: 0.769778\n",
      "Train Epoch: 9 [17920/50000 (36%)]\tLoss: 0.669336\n",
      "Train Epoch: 9 [20480/50000 (41%)]\tLoss: 0.767301\n",
      "Train Epoch: 9 [23040/50000 (46%)]\tLoss: 0.757282\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.845864\n",
      "Train Epoch: 9 [28160/50000 (56%)]\tLoss: 0.901441\n",
      "Train Epoch: 9 [30720/50000 (61%)]\tLoss: 0.676006\n",
      "Train Epoch: 9 [33280/50000 (66%)]\tLoss: 0.776906\n",
      "Train Epoch: 9 [35840/50000 (71%)]\tLoss: 0.830490\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.820042\n",
      "Train Epoch: 9 [40960/50000 (82%)]\tLoss: 0.734799\n",
      "Train Epoch: 9 [43520/50000 (87%)]\tLoss: 0.754214\n",
      "Train Epoch: 9 [46080/50000 (92%)]\tLoss: 0.794345\n",
      "Train Epoch: 9 [48640/50000 (97%)]\tLoss: 0.766496\n",
      "\n",
      "Test set: Average loss: 0.0030, Accuracy: 36916/50000 (73.83%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.709284\n",
      "Train Epoch: 10 [2560/50000 (5%)]\tLoss: 0.753976\n",
      "Train Epoch: 10 [5120/50000 (10%)]\tLoss: 0.761085\n",
      "Train Epoch: 10 [7680/50000 (15%)]\tLoss: 0.744806\n",
      "Train Epoch: 10 [10240/50000 (20%)]\tLoss: 0.698052\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.797952\n",
      "Train Epoch: 10 [15360/50000 (31%)]\tLoss: 0.791750\n",
      "Train Epoch: 10 [17920/50000 (36%)]\tLoss: 0.762738\n",
      "Train Epoch: 10 [20480/50000 (41%)]\tLoss: 0.799306\n",
      "Train Epoch: 10 [23040/50000 (46%)]\tLoss: 0.801378\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.840077\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-d07dc264c2de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-137-d07dc264c2de>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, criterion, epoch, log_interval, dry_run)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch, log_interval=5, dry_run=False):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss\n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "\n",
    "device = \"cuda:0\"\n",
    "lr = 0.01\n",
    "gamma = 0.7\n",
    "epochs = 100\n",
    "model = VerySimpleNet(10).to(device)\n",
    "# model = UNet(3,10).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch, log_interval=10)\n",
    "    test(model, device, criterion, test_loader)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at some classification examples from the test set, how is the performance of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target label: dog, predicted label: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for batch, target in test_loader:\n",
    "    sample = batch[0].permute(1, 2, 0)\n",
    "    output = model(batch.to(device))\n",
    "    p_label = torch.argmax(output[0]).item()\n",
    "    t_label = target[0].item()\n",
    "    print(f\"target label: {label_to_classname[t_label]}, predicted label: {label_to_classname[p_label]}\")\n",
    "    plt.imshow(sample.numpy())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Convnets - Architecture, Backbones & Layers\n",
    "Backbones are large (often pretrained) neural networks that serve as a way to extract features that are useful for almost all types of problems.\n",
    "The general idea of a backbone is to have a model that is very good at identifying high level features in an image. \n",
    "The easiest way to understand this is to think of images as collections of shapes. A face is just a collection of circles (such as the eyes) and curvy lines.\n",
    "This means that if we already have a model that can detect all these components the step to the full combination of those components (such as a face) is a lot easier!\n",
    "In practice most pretrained backbones will already have concepts such as faces embedded into the layers. Additionally, lines and circles are actually pretty basic features and a large pretrained backbone will contain much more complex concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting CIFAR-10 \n",
    "Now we know about backbones, let's use one on the CIFAR-10 dataset. For this we'll need to download a pretrained model and adjust the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "resnext50 = models.resnext50_32x4d(pretrained=True)\n",
    "# model = models.resnext50_32x4d(pretrained=False)\n",
    "\n",
    "# you can always print a model to see the full structure or even the partial structure if you select it.\n",
    "print(resnext50.layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing a backbone\n",
    "We now want to change the backbone so it fits our 10 class classification problem. There are a few ways to do this and here we will look at both of the methods.\n",
    "\n",
    "1. The first (and easiest) way is to change the model is to just swap out the last layer. In this case the original model was already a classification model and we are just changing the number of output classes in the last layer to fit our problem. In this case this is also the most 'correct' way of doing it.\n",
    "2. The second way to adjust the model is to wrap it in a new model class. This method is useful in more complicated scenarios where you want to change more than just the number of outputs. For example if we wanted to use the backbone as the basis for a segmentation model. Now before you ask, yes wrapping the backbone like this preserves the last layer that would be replaced in the other example, luckily this does not effect performance (only memory usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnext50 = models.resnext50_32x4d(pretrained=True)\n",
    "num_classes = 10\n",
    "\n",
    "# change the last layer of the model, (run the training cell further below to see if it worked):\n",
    "resnext50.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "print(resnext50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedResnext(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# now let's wrap our model in a pytorch module instead, this is syntactically the same as adding a layer to a regular network.\n",
    "\n",
    "class WrappedResnext(nn.Module):\n",
    "    def __init__(self, n_classes, backbone):\n",
    "        super(WrappedResnext, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.fc2 = nn.Linear(1000, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "resnext50 = models.resnext50_32x4d(pretrained=True)\n",
    "model = WrappedResnext(10, resnext50)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your adjusted model by running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.906863\n",
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 1.859856\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 1.656146\n",
      "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 1.412381\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 1.368838\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.294761\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.061087\n",
      "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 1.143962\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.074286\n",
      "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.228976\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.086726\n",
      "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 0.778796\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 0.914323\n",
      "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 0.923098\n",
      "Train Epoch: 1 [35840/50000 (71%)]\tLoss: 0.758366\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.766399\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 0.771607\n",
      "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 0.874980\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 0.684470\n",
      "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 0.543703\n",
      "\n",
      "Test set: Average loss: 0.0077, Accuracy: 35948/50000 (71.90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.850775\n",
      "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.031030\n",
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 0.830713\n",
      "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 0.821678\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-d3353f75ac58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-137-d07dc264c2de>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, criterion, epoch, log_interval, dry_run)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-158-66f4e7add8ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch, log_interval=10)\n",
    "    test(model, device, criterion, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Convnets - Model architecture\n",
    "In this section we'll look at Unet, a (compared to more recent models) classic deep learning segmentation model that introduced the concept of encoder-decoder architecture for convnets. It is also fairly easy to understand. It consists of two parts, the encoder and the decoder. The Encoder has the same job as the backbone has in the previous section. Its purpose is to extract features from the image, that will then be used in the later half of the model to make the segmentation. In this analogy the Decoder does the same thing as the single layer of linear nodes in the backbone example. It uses the features supplied by the encoder to make the full image segmentation. \n",
    "\n",
    "Now that we have a general idea, what are the other interesting parts of the model? \n",
    "1. As we can see in the image, the shape of the layers gets deeper in filters but smaller in width/height.\n",
    "This is done to allow the model to learn larger more complex concepts. As the size of the convolutional filters stays the same throughout the model (generally kernel size is always 3x3), a larger object like a car would never fit in those 3 pixels. By downsizing the output after each layer, a full object CAN be represented in that 3x3 grid of pictures. This is because filters specialize, a certain filter in the 4th layer of the model might only respond to a specific pattern that was found in the previous layer. That pattern is again already an abstraction upon the previous input.\n",
    "\n",
    "2. What are those grey lines jumping from the enconder part to the decoder part? As you might have suspected this is just the output of that layer being saved and copied to be concatenated to the decoder at the mirrored layer later. This is done because a lot of spatial detail is lost in the process of downsampling. By adding the higher resolution data from the downsampling process this effect is somewhat mitigated as the network is able to use this data to make more precise object boundries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://miro.medium.com/max/2824/1*f7YOaE4TWubwaFF7Z1fzNw.png \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the segmentation model\n",
    "Fill in the empty functions and complete the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From classification model to segmentation model\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )   \n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)        \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        \n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        \n",
    "def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "        x = self.dconv_down4(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)       \n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "        x = self.dconv_up1(x)\n",
    "        out = self.conv_last(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model \n",
    "Lets now try out our new model on another pytorch/torchvision dataset. We'll use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 86, in default_collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-368083b6849a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                           num_workers=5)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mthing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     sample = batch[0].permute(1, 2, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 86, in default_collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(sbd_dataset,\n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=5)\n",
    "\n",
    "for thing in test_loader:\n",
    "    print(thing)\n",
    "#     sample = batch[0].permute(1, 2, 0)\n",
    "#     output = model(batch.to(device))\n",
    "#     p_label = torch.argmax(output[0]).item()\n",
    "#     t_label = target[0].item()\n",
    "#     print(f\"target label: {label_to_classname[t_label]}, predicted label: {label_to_classname[p_label]}\")\n",
    "#     plt.imshow(sample.numpy())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 86, in default_collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-ec7781bd798a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mthing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/parting/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 86, in default_collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "for thing in test_loader:\n",
    "    print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./sbd/benchmark.tgz\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Destination path './sbd/cls' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-49789585eccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m ])\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msbd_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSBDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./sbd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"segmentation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/site-packages/torchvision/datasets/sbd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, image_set, mode, download, transforms)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"cls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"img\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inst\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val.txt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mold_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_ds_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msbd_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             download_url(self.voc_train_url, sbd_root, self.voc_split_filename,\n\u001b[1;32m     83\u001b[0m                          self.voc_split_md5)\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mreal_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Destination path './sbd/cls' already exists"
     ]
    }
   ],
   "source": [
    "# this dataset is 1.4 Gigabyte so be patient. \n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "sbd_dataset = torchvision.datasets.SBDataset(root=\"./sbd\", download=True, mode=\"segmentation\", image_set='train', transforms=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Convnets - Filter Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CIFAR10',\n",
       " 'CIFAR100',\n",
       " 'Caltech101',\n",
       " 'Caltech256',\n",
       " 'CelebA',\n",
       " 'Cityscapes',\n",
       " 'CocoCaptions',\n",
       " 'CocoDetection',\n",
       " 'DatasetFolder',\n",
       " 'EMNIST',\n",
       " 'FakeData',\n",
       " 'FashionMNIST',\n",
       " 'Flickr30k',\n",
       " 'Flickr8k',\n",
       " 'HMDB51',\n",
       " 'ImageFolder',\n",
       " 'ImageNet',\n",
       " 'KMNIST',\n",
       " 'Kinetics400',\n",
       " 'LSUN',\n",
       " 'LSUNClass',\n",
       " 'MNIST',\n",
       " 'Omniglot',\n",
       " 'PhotoTour',\n",
       " 'QMNIST',\n",
       " 'SBDataset',\n",
       " 'SBU',\n",
       " 'SEMEION',\n",
       " 'STL10',\n",
       " 'SVHN',\n",
       " 'UCF101',\n",
       " 'USPS',\n",
       " 'VOCDetection',\n",
       " 'VOCSegmentation',\n",
       " 'VisionDataset',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'caltech',\n",
       " 'celeba',\n",
       " 'cifar',\n",
       " 'cityscapes',\n",
       " 'coco',\n",
       " 'fakedata',\n",
       " 'flickr',\n",
       " 'folder',\n",
       " 'hmdb51',\n",
       " 'imagenet',\n",
       " 'kinetics',\n",
       " 'lsun',\n",
       " 'mnist',\n",
       " 'omniglot',\n",
       " 'phototour',\n",
       " 'sbd',\n",
       " 'sbu',\n",
       " 'semeion',\n",
       " 'stl10',\n",
       " 'svhn',\n",
       " 'ucf101',\n",
       " 'usps',\n",
       " 'utils',\n",
       " 'video_utils',\n",
       " 'vision',\n",
       " 'voc']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torchvision.datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convnets for spatially related non-image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/#downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch16]",
   "language": "python",
   "name": "conda-env-pytorch16-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
